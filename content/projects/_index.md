---
title: "Yuxuan Wu - Projects"
date: 2022-10-03T08:45:26-04:00
draft: false
---

<head>
  <meta charset="utf-8">
  <link rel="stylesheet" href="card.css" media="all">
</head>



<!-- ## Motif Representation Learning, Master Thesis -->
<div class="card">
    <div class="card-image" style="background-image: url(motif.png)"></div>
    <div class="card-content">
        <h1>Motif-based Music Representation Learning</h1>
        <p>The formation of music structure heavily relies on repetitions and variations of music motifs. Understanding the manifestations and behaviors of these motifs is crucial for effective music structure analysis and high-quality automatic music composition. However, capturing music motifs' implicit nature is often challenging. In this study, we employ deep learning techniques to explore an efficacious method for learning robust representations of music motifs.</p>
        <div class="card-details">
        <div class="card-details-inner">
            <div class="read-more">
            <a class="button" href="">Building...</a>
            </div>
        </div>
        </div>
    </div>
</div>
<br />

<!-- ## The Jazz Tutor -->
<div class="card">
    <div class="card-image" style="background-image: url(jazztutor.jpg)"></div>
    <div class="card-content">
        <h1>The Jazz Tutor</h1>
        <p>We use generative models and careful HCI design to help improve novices' practicing experience/efficiency on jazz improvisation.</p>
        <div class="card-details">
        <div class="card-details-inner">
            <div class="read-more">
            <a class="button" href="">Building...</a>
            </div>
        </div>
        </div>
    </div>
</div>
<br />

<!-- ## Expressive Performance Generation -->
<!-- <div class="card">
    <div class="card-image" style="background-image: url(performance.jpg)"></div>
    <div class="card-content">
        <h1>Expressive Performance Generation</h1>
        <p>To generate stylistic expressive performance from MIDI, we first do performance analysis on the control signal level, and then use generative models to generate control signals. In the end, we will use an instrument model to synthesize music performance.</p>
        <div class="card-details">
        <div class="card-details-inner">
            <div class="read-more">
            <a class="button" href="">Building...</a>
            </div>
        </div>
        </div>
    </div>
</div>
<br /> -->

<!-- ## Timbre Transfer with Flexible Timbre Control  -->
<div class="card">
    <div class="card-image" style="background-image: url(transplayer.png)"></div>
    <div class="card-content">
        <h1>Timbre Transfer with Flexible Timbre Control</h1>
        <p>Timbre style transfer has been an intriguing but mysterious sub-topic in music style transfer. We use a concise autoencoder model with one-hot representations of instruments as the condition, and a Diffwave model trained especially for music synthesis. The results proved that our method is able to provide one-to-one style transfer outputs comparable with the existing GAN-based method, and can transfer among multiple timbres with only one single model.</p>
        <div class="card-details">
        <div class="card-details-inner">
            <div class="read-more">
            <a class="button" href="https://ieeexplore.ieee.org/document/10096233">Paper</a>
            </div>
        </div>
        </div>
    </div>
</div>
<br />

<!-- ## A to I -->
<div class="card">
    <div class="card-image" style="background-image: url(3+i.jpg)"></div>
    <div class="card-content">
        <h1><i>A to I</i></h1>
        <div class="subtitle">For AI Song Contest 2022, with team 3+i</div>
        <p>Nowadays, AI models excel with impressive performance on various tasks that were once only considered humans-exclusive. As AI models grow more powerful, their role in co-creation expands. However, ethical concerns arise with the rise of AI: will AI models replace or be harmful to humans? In this song, we try to answer those open questions by exploring the possible roles AI models could play in the co-creation process and try to resolve the ethical concern from the perspective of AI themselves. The AI models in this song act not only as tools but also as collaborators, song and lyrics writers, performers, storytellers, and even the mentor and first-person narrator.</p>
        <div class="card-details">
        <div class="card-details-inner">
            <div class="read-more">
            <a class="button" href="https://www.aisongcontest.com/participants-2022/3i">Project Page</a>
            </div>
        </div>
        </div>
    </div>
</div>
<br />

<!-- ## Speech Anonymization  -->
<div class="card">
    <div class="card-image" style="background-image: url(deid_vc.jpg)"></div>
    <div class="card-content">
        <h1>Speech Anonymization with Pseudo Voice Conversion</h1>
        <div class="subtitle"></div>
        <p>The widespread adoption of speech-based online services raises security and privacy concerns regarding the data that they use and share. If the data were compromised, attackers could exploit user speech to bypass speaker verification systems or even impersonate users. To mitigate this, we propose DeID-VC, a speaker de-identification system that converts a real speaker to pseudo speakers, thus removing or obfuscating the speaker-dependent attributes from a spoken voice.</p>
        <div class="card-details">
        <div class="card-details-inner">
            <div class="read-more">
            <a class="button" href="https://arxiv.org/pdf/2209.04530.pdf">Paper</a>
            </div>
        </div>
        </div>
    </div>
</div>
<br />

<!-- ## Project Ming -->
<div class="card">
    <div class="card-image" style="background-image: url(project_ming.png)"></div>
    <div class="card-content">
        <h1>Project Ming</h1>
        <div class="subtitle"></div>
        <p>Project Ming is a Cycling 74 Max/MSP program that can simulate the sound ambience in ancient Chinese cities. By moving your mouse on the map and pressing different keys on the keyboard, you can experience through the colorful and realistic soundscape in ancient China. It was completed during my time at Berkeley summer school in 2019, and a variety of synthesizing techniques were adopted.</p>
        <div class="card-details">
        <div class="card-details-inner">
            <div class="read-more">
            <a class="button" href="https://drive.google.com/file/d/1RO0CLQU-DuB6bjO2We0h0V2WVvcp9bqA/view?usp=sharing">Video Demo</a>
            </div>
        </div>
        </div>
    </div>
</div>
<br />
